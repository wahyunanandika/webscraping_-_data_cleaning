{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "years = list(range(2025, 2026))  # Define the range of years for data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_player_stats(years: list, player_url: str, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Scrapes per-game player statistics from Basketball Reference.\n",
    "    \n",
    "    Args:\n",
    "        years (list): A list of years to scrape data for.\n",
    "        player_url (str): The URL format for each player's page, where the year will be inserted.\n",
    "        filename (str): The name of the file to save the scraped data to.\n",
    "    \n",
    "    Returns:\n",
    "        None: Saves the scraped data as a CSV file.\n",
    "    \"\"\"\n",
    "    dataframes = []  # Initialize an empty list to store data for each year\n",
    "\n",
    "    for year in years:  # Loop through each year to scrape data\n",
    "        service = Service(\"chromedriver.exe\")  # Set up the ChromeDriver service\n",
    "        driver = webdriver.Chrome(service=service)  # Start a new Chrome browser instance\n",
    "        url = player_url.format(year)  # Format the URL by inserting the current year\n",
    "        driver.get(url)  # Open the webpage\n",
    "        driver.execute_script(\"window.scrollTo(1,100000)\")  # Scroll to the bottom of the page to load all content\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')  # Parse the page content with BeautifulSoup\n",
    "        player_table = soup.find(id='per_game_stats')  # Locate the table containing per-game stats\n",
    "\n",
    "        if player_table:  # Check if the table is found\n",
    "            player = pd.read_html(str(player_table))[0]  # Convert the HTML table to a Pandas DataFrame\n",
    "            player['year'] = year  # Add a column to indicate the season year\n",
    "            player = player[(player['Player'] != 'Player') & ~player['Player'].str.contains('League', na=False, case=False)]  \n",
    "            # Remove unnecessary rows where 'Player' column contains 'Player' (header) or 'League'\n",
    "            dataframes.append(player)  # Add the cleaned DataFrame to the list\n",
    "\n",
    "        time.sleep(3)  # Pause execution for 3 seconds to prevent overwhelming the website\n",
    "        driver.quit()  # Close the browser instance\n",
    "\n",
    "    final_df = pd.concat(dataframes, ignore_index=True)  # Combine all DataFrames into one\n",
    "    final_df.to_csv(filename, index=False)  # Save the final DataFrame as a CSV file without an index\n",
    "\n",
    "def scrape_team_stats(years: list, team_url: str, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Scrapes team standings and saves the cleaned data to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        years (list): A list of years to scrape data for.\n",
    "        team_url (str): The URL format for each team's page, where the year will be inserted.\n",
    "        filename (str): The name of the file to save the scraped data to.\n",
    "    \n",
    "    Returns:\n",
    "        None: Saves the scraped data as a CSV file.\n",
    "    \"\"\"\n",
    "    team_dfs = []  # Initialize an empty list to store team standings data\n",
    "\n",
    "    for year in years:  # Loop through each year to scrape data\n",
    "        url = team_url.format(year)  # Format the URL by inserting the current year\n",
    "        data = requests.get(url)  # Send an HTTP request to fetch the webpage content\n",
    "        soup = BeautifulSoup(data.text, 'html.parser')  # Parse the page content with BeautifulSoup\n",
    "\n",
    "        for conf in ['E', 'W']:  # Loop through both Eastern and Western Conference standings\n",
    "            team_table = soup.find(id=f'divs_standings_{conf}')  # Find the table for the respective conference\n",
    "\n",
    "            if team_table:  # Check if the table exists on the page\n",
    "                team = pd.read_html(str(team_table))[0]  # Convert the HTML table to a Pandas DataFrame\n",
    "                team['year'] = year  # Add a column to indicate the season year\n",
    "                team['team'] = team.iloc[:, 0]  # Extract the team names from the first column\n",
    "                team = team[~team['team'].str.contains('Division')]  # Remove rows containing 'Division' headers\n",
    "\n",
    "                if conf == 'E':  \n",
    "                    del team['Eastern Conference']  # Remove the 'Eastern Conference' column if present\n",
    "                elif conf == 'W':\n",
    "                    del team['Western Conference']  # Remove the 'Western Conference' column if present\n",
    "\n",
    "                team_dfs.append(team)  # Add the cleaned DataFrame to the list\n",
    "\n",
    "        time.sleep(3)  # Pause execution for 3 seconds to prevent overwhelming the website\n",
    "\n",
    "    all_teams = pd.concat(team_dfs)  # Combine all DataFrames into one\n",
    "    all_teams.to_csv(filename, index=False)  # Save the final DataFrame as a CSV file without an index\n",
    "\n",
    "def scrape_advance_stats(years: list, advance_url: str, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Scrapes advanced player statistics and saves the data to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        years (list): A list of years to scrape data for.\n",
    "        advance_url (str): The URL format for each year's advanced stats page.\n",
    "        filename (str): The name of the file to save the scraped data to.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe containing the scraped advanced stats for all years.\n",
    "    \"\"\"\n",
    "    advance_df = []  # Initialize an empty list to store scraped data\n",
    "\n",
    "    for year in years:  # Loop through each year to scrape data\n",
    "        service = Service(\"chromedriver.exe\")  # Set up the ChromeDriver service\n",
    "        driver = webdriver.Chrome(service=service)  # Launch a new Chrome instance\n",
    "        url = advance_url.format(year)  # Format the URL by inserting the current year\n",
    "        driver.get(url)  # Open the URL in the browser\n",
    "        driver.execute_script(\"window.scrollTo(1,100000)\")  # Scroll down to ensure the full page loads\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')  # Parse the page content with BeautifulSoup\n",
    "        player_table = soup.find(id='advanced')  # Locate the advanced stats table\n",
    "\n",
    "        if player_table:  # Check if the table exists on the page\n",
    "            player = pd.read_html(str(player_table))[0]  # Convert the HTML table to a Pandas DataFrame\n",
    "            player['year'] = year  # Add a column to indicate the season year\n",
    "\n",
    "            # Remove redundant header rows and league average rows\n",
    "            player = player[(player['Player'] != 'Player') & ~player['Player'].str.contains('League', na=False, case=False)]\n",
    "\n",
    "            advance_df.append(player)  # Add the cleaned DataFrame to the list\n",
    "\n",
    "        time.sleep(3)  # Pause execution for 3 seconds to avoid overloading the website\n",
    "        driver.quit()  # Close the browser session\n",
    "\n",
    "    final_df = pd.concat(advance_df, ignore_index=True)  # Combine all DataFrames into one\n",
    "    final_df.to_csv(filename, index=False)  # Save the final DataFrame as a CSV file without an index\n",
    "\n",
    "\n",
    "def process_players(players: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensures only one row per player per year, keeping the last team played.\n",
    "    \n",
    "    Args:\n",
    "        players (pd.DataFrame): The dataframe containing player data with potential multiple rows for each player.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with a single row per player per year.\n",
    "    \"\"\"\n",
    "\n",
    "    def single_row(df):  \n",
    "        \"\"\"\n",
    "        Keeps only one row per player per year. If a player played for multiple teams, \n",
    "        assigns the last team's abbreviation while keeping the aggregated (TOT) row.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): A subset of the dataframe for a specific player in a specific year.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: A dataframe with only one row per player per year.\n",
    "        \"\"\"\n",
    "        return df if df.shape[0] == 1 else df[df['Tm'].str.contains(r'TOT|TM', na=False)].assign(Tm=df.iloc[-1]['Tm'])\n",
    "\n",
    "    players = players.groupby(['Player', 'year']).apply(single_row)  # Group by player and year, apply the function\n",
    "    players.index = players.index.droplevel([0, 1])  # Drop the multi-index created by groupby\n",
    "\n",
    "    return players  # Return the processed DataFrame\n",
    "\n",
    "def load_advance(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads and cleans advanced player statistics.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): The path to the CSV file containing advanced stats.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned dataframe with advanced player stats.\n",
    "    \"\"\"\n",
    "    \n",
    "    adv_stats = pd.read_csv(filepath)  # Load the CSV file into a DataFrame\n",
    "    \n",
    "    adv_stats.drop(columns=['Rk', 'Awards'], inplace=True)  # Remove unnecessary columns\n",
    "    \n",
    "    adv_stats.rename(columns={'Team': 'Tm', 'MP': 'TMP'}, inplace=True)  # Rename columns for consistency\n",
    "    \n",
    "    adv_stats['Player'] = adv_stats['Player'].str.replace('*', '', regex=False)  # Remove asterisks from player names\n",
    "    \n",
    "    adv_stats.fillna(0, inplace=True)  # Replace missing values with 0\n",
    "    \n",
    "    adv_stats = process_players(adv_stats)  # Ensure each player has only one row per year\n",
    "    \n",
    "    # Select relevant columns for the final cleaned DataFrame\n",
    "    return adv_stats[['Player', 'year', 'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%',\n",
    "                      'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP']]\n",
    "\n",
    "\n",
    "def load_players(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads and cleans player data.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): The path to the CSV file containing player stats.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned dataframe with player stats.\n",
    "    \"\"\"\n",
    "    \n",
    "    players = pd.read_csv(filepath)  # Load the player stats CSV into a DataFrame\n",
    "    \n",
    "    players.drop(columns=['Rk', 'Awards'], inplace=True, errors='ignore')  # Drop unnecessary columns, ignoring errors if columns are missing\n",
    "    \n",
    "    players.rename(columns={'Team': 'Tm'}, inplace=True)  # Rename the 'Team' column to 'Tm' for consistency\n",
    "    \n",
    "    players['Player'] = players['Player'].str.replace('*', '', regex=False)  # Remove asterisks from player names (indicating special players)\n",
    "    \n",
    "    return process_players(players)  # Process the player data to ensure only one row per player per year\n",
    "\n",
    "def load_teams(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads and cleans team data.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): The path to the CSV file containing team data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned dataframe with team data.\n",
    "    \"\"\"\n",
    "    \n",
    "    teams = pd.read_csv(filepath)  # Load the team data from the specified CSV file\n",
    "    \n",
    "    teams['team'] = teams['team'].str.replace('*', '', regex=False)  # Remove asterisks from team names (indicating special teams)\n",
    "    \n",
    "    teams['GB'] = teams['GB'].str.replace('—', '0')  # Replace the '—' symbol with '0' in the 'GB' column (games behind)\n",
    "    \n",
    "    teams.drop(columns=['Unnamed: 0'], inplace=True, errors='ignore')  # Drop any unnamed columns (e.g., index column) if they exist\n",
    "    \n",
    "    teams['team'] = teams['team'].str.replace(r'\\s\\(\\d+\\)', '', regex=True)  # Remove any numbers in parentheses (e.g., team abbreviation or city code)\n",
    "    \n",
    "    return teams  # Return the cleaned team data\n",
    "\n",
    "def load_nicknames(filepath: str) -> dict:\n",
    "    \"\"\"\n",
    "    Loads team nickname mappings.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): The path to the CSV file containing team nickname mappings.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary mapping team abbreviations to their full names.\n",
    "    \"\"\"\n",
    "    \n",
    "    nicknames = {}  # Initialize an empty dictionary to store team nickname mappings\n",
    "    \n",
    "    with open(filepath) as f:  # Open the file specified by the filepath\n",
    "        lines = f.readlines()  # Read all the lines in the file\n",
    "        for line in lines[1:]:  # Skip the header (first line) and process the remaining lines\n",
    "            team, abbreviation = line.strip().split(',')  # Split each line into team and abbreviation\n",
    "            nicknames[abbreviation] = team  # Add the abbreviation and team name to the dictionary\n",
    "    \n",
    "    return nicknames  # Return the dictionary containing team abbreviations and their full names\n",
    "\n",
    "def merge_data(players: pd.DataFrame, teams: pd.DataFrame, advanced_stats: pd.DataFrame, nicknames: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merges player stats with team stats.\n",
    "    \n",
    "    Args:\n",
    "        players (pd.DataFrame): The dataframe containing player stats.\n",
    "        teams (pd.DataFrame): The dataframe containing team stats.\n",
    "        advanced_stats (pd.DataFrame): The dataframe containing advanced player stats.\n",
    "        nicknames (dict): A dictionary mapping team abbreviations to their full names.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A merged dataframe with player and team stats.\n",
    "    \"\"\"\n",
    "    \n",
    "    players['team'] = players['Tm'].map(nicknames)  # Map the team abbreviations to full team names using the 'nicknames' dictionary\n",
    "    \n",
    "    merged = players.merge(teams, how='outer', on=['team', 'year'])  # Merge the player data with team data based on 'team' and 'year', using an outer join to keep all records\n",
    "    \n",
    "    merged = merged.merge(advanced_stats, on=['Player', 'year'], how='inner')  # Merge the resulting dataframe with advanced stats based on 'Player' and 'year', using an inner join\n",
    "    \n",
    "    for col in ['FG%', '3P%', '2P%', 'eFG%', 'FT%']:  # Iterate through a list of column names\n",
    "        merged[col] = merged[col].fillna(0)  # Fill any missing values in the columns with 0\n",
    "    \n",
    "    new_columns = ['Player', 'Pos', 'Age', 'Tm', 'G', 'GS', 'MP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', '2P', '2PA', '2P%', 'eFG%', 'FT', 'FTA', 'FT%', \n",
    "                   'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PF', 'PTS', 'year', 'PER', 'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', \n",
    "                   'BLK%', 'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM', 'VORP', 'Pts Won', 'Pts Max', 'Share', 'team', 'W', 'L', 'W/L%', \n",
    "                   'GB', 'PS/G', 'PA/G', 'SRS']  # List of new columns that should be present in the final dataframe\n",
    "    \n",
    "    # Add the new columns with default value 0\n",
    "    merged = merged.reindex(columns=new_columns)  # Reindex the dataframe to ensure all the specified columns are included\n",
    "    \n",
    "    merged[['Pts Won', 'Pts Max', 'Share']] = 0  # Set the values of the 'Pts Won', 'Pts Max', and 'Share' columns to 0\n",
    "    \n",
    "    return merged  # Return the merged dataframe\n",
    "\n",
    "def save_final_data(df: pd.DataFrame, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves final cleaned data to CSV.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing the final merged data.\n",
    "        output_path (str): The path where the cleaned data should be saved as a CSV.\n",
    "    \n",
    "    Returns:\n",
    "        None: Saves the dataframe to a CSV file.\n",
    "    \"\"\"\n",
    "    df.to_csv(output_path, index=False)  # Save the dataframe to a CSV file, excluding the index\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wahyunan\\AppData\\Local\\Temp\\ipykernel_26100\\130466608.py:25: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  player = pd.read_html(str(player_table))[0]  # Convert the HTML table to a Pandas DataFrame\n",
      "C:\\Users\\Wahyunan\\AppData\\Local\\Temp\\ipykernel_26100\\130466608.py:60: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  team = pd.read_html(str(team_table))[0]  # Convert the HTML table to a Pandas DataFrame\n",
      "C:\\Users\\Wahyunan\\AppData\\Local\\Temp\\ipykernel_26100\\130466608.py:60: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  team = pd.read_html(str(team_table))[0]  # Convert the HTML table to a Pandas DataFrame\n",
      "C:\\Users\\Wahyunan\\AppData\\Local\\Temp\\ipykernel_26100\\130466608.py:102: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  player = pd.read_html(str(player_table))[0]  # Convert the HTML table to a Pandas DataFrame\n",
      "C:\\Users\\Wahyunan\\AppData\\Local\\Temp\\ipykernel_26100\\130466608.py:141: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  players = players.groupby(['Player', 'year']).apply(single_row)  # Group by player and year, apply the function\n",
      "C:\\Users\\Wahyunan\\AppData\\Local\\Temp\\ipykernel_26100\\130466608.py:141: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  players = players.groupby(['Player', 'year']).apply(single_row)  # Group by player and year, apply the function\n"
     ]
    }
   ],
   "source": [
    "# Initialization and Execution\n",
    "player_url = 'https://www.basketball-reference.com/leagues/NBA_{}_per_game.html'\n",
    "team_url = 'https://www.basketball-reference.com/leagues/NBA_{}_standings.html'\n",
    "adv_stat_url = 'https://www.basketball-reference.com/leagues/NBA_{}_advanced.html'\n",
    "player_filename = 'player_2025.csv'\n",
    "advance_stat_filename = 'player_stats_2025.csv'\n",
    "team_filename = 'all_teams_2025.csv'\n",
    "nickname_path = 'nicknames.csv'\n",
    "output_path = 'final_cleaned_data.csv'\n",
    "\n",
    "scrape_player_stats(years, player_url, player_filename)\n",
    "scrape_team_stats(years, team_url, team_filename)\n",
    "scrape_advance_stats(years,adv_stat_url,advance_stat_filename)\n",
    "players = load_players(player_filename)\n",
    "advance_stat = load_advance(advance_stat_filename)\n",
    "teams = load_teams(team_filename)\n",
    "nicknames = load_nicknames(nickname_path)\n",
    "final_data = merge_data(players, teams, advance_stat,nicknames)\n",
    "save_final_data(final_data, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(533, 61)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SG', 'C', 'PF', 'SF', 'PG'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data['Pos'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: bool)\n"
     ]
    }
   ],
   "source": [
    "missing_values = final_data.isnull().any()\n",
    "\n",
    "# Displaying the columns with missing values\n",
    "print(missing_values[missing_values == True])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
